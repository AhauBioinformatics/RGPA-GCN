{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, time, math\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.data import register_data_args\n",
    "#from dgl.data import CoraGraphDataset, CiteseerGraphDataset, PubmedGraphDataset\n",
    "\n",
    "\n",
    "def gcn_msg(edge):\n",
    "    msg = edge.src['h'] * edge.src['norm']\n",
    "    return {'m': msg}\n",
    "\n",
    "\n",
    "def gcn_reduce(node):\n",
    "    accum = torch.sum(node.mailbox['m'], 1) * node.data['norm']\n",
    "    return {'h': accum}\n",
    "\n",
    "\n",
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, out_feats, activation=torch.sigmoid, bias=True):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_feats))\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.activation = activation\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.bias is not None:\n",
    "            stdv = 1. / math.sqrt(self.bias.size(0))\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        h = nodes.data['h']\n",
    "        if self.bias is not None:\n",
    "            h = h + self.bias\n",
    "        if self.activation:\n",
    "            h = self.activation(h)\n",
    "        return {'h': h}\n",
    "\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 bias=True):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.g = g\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_feats, out_feats))\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "        self.node_update = NodeApplyModule(out_feats, activation, bias)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, h):\n",
    "        if self.dropout:\n",
    "            h = self.dropout(h)\n",
    "        self.g.ndata['h'] = torch.mm(h, self.weight)\n",
    "        self.g.update_all(gcn_msg, gcn_reduce, self.node_update)\n",
    "        h = self.g.ndata.pop('h')\n",
    "        return h\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GCNLayer(g, in_feats, n_hidden, activation, dropout))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GCNLayer(g, n_hidden, n_hidden, activation, dropout))\n",
    "        # output layer\n",
    "        self.layers.append(GCNLayer(g, n_hidden, n_classes, None, dropout))\n",
    "\n",
    "        #\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "#         for i in range(len(self.layers)-1):\n",
    "#             layer = self.layers[i]\n",
    "#             h = layer(h)\n",
    "#             if i == 2:\n",
    "#                 print(h.shape)\n",
    "#                 np.savetxt('gcn_5_0.txt',h.detach().numpy(),delimiter=' ', newline='\\n',)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        probas = F.softmax(logits)\n",
    "        \n",
    "#         correct = torch.sum(indices == labels)\n",
    "#         return correct.item() * 1.0 / len(labels)\n",
    "        return metrics(labels.cpu().detach(), indices.cpu().detach(), probas.cpu().detach()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_true, y_pred, y_prob):\n",
    "\n",
    "    y_true, y_pred, y_prob = y_true.numpy(), y_pred.numpy(), y_prob.numpy()\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    pos_acc = tp / sum(y_true)\n",
    "    neg_acc = tn / (len(y_pred) - sum(y_pred)) # [y_true=0 & y_pred=0] / y_pred=0\n",
    "    accuracy = (tp+tn)/(tn+fp+fn+tp)\n",
    "    \n",
    "    recall = tp / (tp+fn)\n",
    "    precision = tp / (tp+fp)\n",
    "    f1 = 2*precision*recall / (precision+recall)\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    prec, reca, _ = precision_recall_curve(y_true, y_prob)\n",
    "    aupr = auc(reca, prec)\n",
    "    f = open('result4.txt','a', encoding='utf-8')\n",
    "    f.writelines('tn = {}, fp = {}, fn = {}, tp = {} \\n'.format(tn, fp, fn, tp))\n",
    "    f.writelines('y_pred: 0 = {} | 1 = {}\\n'.format(Counter(y_pred)[0], Counter(y_pred)[1]))\n",
    "    f.writelines('y_true: 0 = {} | 1 = {}\\n'.format(Counter(y_true)[0], Counter(y_true)[1]))\n",
    "    f.writelines('acc={:.4f}|precision={:.4f}|recall={:.4f}|f1={:.4f}|auc={:.4f}|aupr={:.4f}|pos_acc={:.4f}|neg_acc={:.4f}\\n'.format(accuracy, precision, recall, f1, roc_auc, aupr, pos_acc, neg_acc))\n",
    "    f.close()\n",
    "#     np.savetxt(\"result.txt\",'tn = {}, fp = {}, fn = {}, tp = {} \\n'.format(tn, fp, fn, tp))\n",
    "#     np.savetxt(\"result.txt\",'y_pred: 0 = {} | 1 = {}\\n'.format(Counter(y_pred)[0], Counter(y_pred)[1]))\n",
    "#     np.savetxt(\"result.txt\",'y_true: 0 = {} | 1 = {}\\n'.format(Counter(y_true)[0], Counter(y_true)[1]))\n",
    "#     np.savetxt(\"result.txt\",'acc={:.4f}|precision={:.4f}|recall={:.4f}|f1={:.4f}|auc={:.4f}|aupr={:.4f}|pos_acc={:.4f}|neg_acc={:.4f}\\n'.format(accuracy, precision, recall, f1, roc_auc, aupr, pos_acc, neg_acc))\n",
    "    return (y_true, y_pred, y_prob), (accuracy, precision, recall, f1, roc_auc, aupr, pos_acc, neg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "from copy import deepcopy\n",
    "import warnings \n",
    "import os\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args, g, features, labels, train_idx):\n",
    "\n",
    "    if args.gpu >= 0:\n",
    "        cuda = True\n",
    "        device = torch.device('cuda:%d' % args.gpu)\n",
    "    else:\n",
    "        cuda = False\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    num_nodes = g.number_of_nodes()\n",
    "    train_mask = np.zeros(num_nodes, dtype = 'int64')\n",
    "    train_mask[train_idx] = 1\n",
    "    test_mask = 1 - train_mask\n",
    "    print(Counter(train_mask), Counter(test_mask))\n",
    "    train_mask = torch.BoolTensor(train_mask)\n",
    "    test_mask = torch.BoolTensor(test_mask)\n",
    "\n",
    "    g.ndata['feat'] = features\n",
    "    g.ndata['label'] = labels\n",
    "    g.ndata['train_mask'] = train_mask\n",
    "    g.ndata['test_mask'] = test_mask\n",
    "    \n",
    "    g = g.to(device)\n",
    "        \n",
    "    in_feats = features.shape[1]\n",
    "    n_classes = 2\n",
    "    n_edges = g.number_of_edges()\n",
    "\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    \n",
    "    print(\"\"\"----Data statistics------'\n",
    "      #Edges %d\n",
    "      #Classes %d\n",
    "      #Train samples %d\n",
    "      #Test samples %d\"\"\" %\n",
    "          (n_edges, n_classes,\n",
    "          train_mask.int().sum().item(),\n",
    "          test_mask.int().sum().item()))\n",
    "\n",
    "    # normalization\n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "    if cuda:\n",
    "        norm = norm.cuda()\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    # create GCN model\n",
    "    model = GCN(g,\n",
    "                in_feats,\n",
    "                args.n_hidden,\n",
    "                n_classes,\n",
    "                args.n_layers,\n",
    "                F.relu,\n",
    "                args.dropout)\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # use optimizer\n",
    "    optimizer = torch.optim.Adamax(model.parameters(),\n",
    "                                 lr=args.lr,\n",
    "                                 weight_decay=args.weight_decay)\n",
    "\n",
    "    # initialize graph\n",
    "    dur = []\n",
    "    for epoch in range(args.n_epochs):\n",
    "        model.train()\n",
    "\n",
    "        t0 = time.time()\n",
    "        # forward\n",
    "        logits = model(features)\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "        loss.requires_grad_(True)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        dur.append(time.time() - t0)\n",
    "        \n",
    "        #print('=====Epoch {} | Time(s) {:.4f} | Loss {:.4f} | ETputs(KTEPS) {:.2f}'.format(epoch, np.mean(dur), loss.item(), n_edges / np.mean(dur) / 1000))\n",
    "        \n",
    "        ys_train, metrics_train = evaluate(model, features, labels, train_mask)\n",
    "        ys_test, metrics_test = evaluate(model, features, labels, test_mask)\n",
    "        \n",
    "    return ys_train, metrics_train, ys_test, metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args, task, isbalance, balance, n_neigh):\n",
    "    pwd = r'D:/小麦/MDA-GCNFTG-main/MDA-GCNFTG-main/0_data/'\n",
    "    \n",
    "    if isbalance:\n",
    "        node_feature_label = pd.read_csv(pwd + 'node_feature_label.csv', index_col = 0)\n",
    "    else:\n",
    "        node_feature_label = pd.read_csv(pwd + 'node_feature_label__nobalance.csv', index_col = 0)\n",
    "    \n",
    "    train_test_id_idx = np.load(r'D:/小麦/MDA-GCNFTG-main/MDA-GCNFTG-main/data/task_' + task + balance + '__testlabel0_knn_edge_train_test_index_all.npz', allow_pickle = True)\n",
    "    train_index_all = train_test_id_idx['train_index_all']\n",
    "    test_index_all = train_test_id_idx['test_index_all']\n",
    "#     绘图\n",
    "    train_idx = train_index_all[0]\n",
    "    test_idx = train_index_all[0]\n",
    "    \n",
    "    num_nodes = node_feature_label.shape[0]\n",
    "    features = torch.FloatTensor(np.array(node_feature_label.iloc[:, 3:]))\n",
    "    labels = torch.LongTensor(np.array(node_feature_label['label']))\n",
    "    \n",
    "    fold = 1\n",
    "    #for train_idx, test_idx in zip(train_index_all, test_index_all):\n",
    "        \n",
    "    print('=====Fold {}============================================='.format(fold))\n",
    "    f = open('result4.txt','a', encoding='utf-8')\n",
    "    f.writelines('=====Fold {}=============================================\\n'.format(fold))\n",
    "    f.close()\n",
    "    knn_graph_file = 'task_' + task + balance + '__testlabel0_knn' + str(n_neigh) + 'neighbors_edge__fold' + str(fold) + '.npz'\n",
    "    knn_neighbors_graph = sp.load_npz(pwd + knn_graph_file)\n",
    "\n",
    "    edge_src = knn_neighbors_graph.nonzero()[0]\n",
    "    edge_dst = knn_neighbors_graph.nonzero()[1]\n",
    "\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(num_nodes)\n",
    "    g.add_edges(edge_src, edge_dst)\n",
    "    g = dgl.add_self_loop(g)\n",
    "    print(g)\n",
    "\n",
    "    ys_train, metrics_train, ys_test, metrics_test = main(args, g, features, labels, train_idx)\n",
    "#         print(metrics_test)\n",
    "    fold += 1\n",
    "    \n",
    "    return node_feature_label, train_index_all, test_index_all, knn_neighbors_graph, g, ys_train, metrics_train, ys_test, metrics_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset=None, dropout=0.3, gpu=-1, lr=0.003, n_epochs=250, n_hidden=512, n_layers=3, weight_decay=0.0005)\n",
      "************** isbalance = True | task = Tp | n_neigh = 5\n",
      "=====Fold 1=============================================\n",
      "Graph(num_nodes=47116, num_edges=282696,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})\n",
      "Counter({1: 37692, 0: 9424}) Counter({0: 37692, 1: 9424})\n",
      "----Data statistics------'\n",
      "      #Edges 282696\n",
      "      #Classes 2\n",
      "      #Train samples 37692\n",
      "      #Test samples 9424\n"
     ]
    }
   ],
   "source": [
    "# balance data\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='GCN')\n",
    "    register_data_args(parser)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.3,\n",
    "            help=\"dropout probability\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=-1,\n",
    "            help=\"gpu\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.003,\n",
    "            help=\"learning rate\")\n",
    "    #一开始是500，画图设置为250\n",
    "    parser.add_argument(\"--n-epochs\", type=int, default=250,\n",
    "            help=\"number of training epochs\")\n",
    "    parser.add_argument(\"--n-hidden\", type=int, default=512,\n",
    "            help=\"number of hidden gcn units\")\n",
    "    parser.add_argument(\"--n-layers\", type=int, default=3,\n",
    "            help=\"number of hidden gcn layers\")\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=5e-4,\n",
    "            help=\"Weight for L2 loss\")\n",
    "    args = parser.parse_args(args = [])\n",
    "    print(args)\n",
    "    \n",
    "    for isbalance in [True]:\n",
    "        \n",
    "        if isbalance:\n",
    "            balance = ''\n",
    "        else:\n",
    "            balance = '__nobalance'\n",
    "        #tp 7 的第三折断的   \n",
    "        for task in ['Tp']:\n",
    "            \n",
    "            for n_neigh in [15]:\n",
    "                \n",
    "                print('************** isbalance = {} | task = {} | n_neigh = {}'.format(isbalance, task, n_neigh))\n",
    "                f = open('result4.txt','a', encoding='utf-8')\n",
    "                f.writelines('************** isbalance = {} | task = {} | n_neigh = {}\\n'.format(isbalance, task, n_neigh))\n",
    "                f.close()\n",
    "                node_feature_label, train_index_all, test_index_all, \\\n",
    "                knn_neighbors_graph, g, ys_train, metrics_train, ys_test, metrics_test = run(args,\n",
    "                                                                                              task, \n",
    "                                                                                              isbalance, \n",
    "                                                                                              balance, \n",
    "                                                                                              n_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
